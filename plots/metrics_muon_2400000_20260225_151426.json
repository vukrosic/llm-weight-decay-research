{
  "final_metrics": {
    "val_loss": 6.149031481742859,
    "val_accuracy": 0.17111321446018563,
    "val_perplexity": 468.26364519985043,
    "train_loss": 6.224682807922363
  },
  "setup_time_seconds": 1.114915370941162,
  "active_training_time_seconds": 76.96240162849426,
  "total_wall_time_seconds": 78.07731699943542,
  "total_time_minutes": 1.301288616657257,
  "actual_steps": 147,
  "tokens_seen": 2408448,
  "train_tokens": 2400000,
  "experiment_config": {
    "optimizer_type": "muon",
    "muon_lr": 0.024,
    "muon_weight_decay": -0.01,
    "adamw_lr": 0.0005,
    "batch_size": 8,
    "gradient_accumulation_steps": 1
  },
  "history": {
    "steps": [
      0,
      50,
      100,
      147
    ],
    "val_losses": [
      10.884679203033448,
      6.663510680198669,
      6.3595586681365965,
      6.149031481742859
    ],
    "val_accuracies": [
      0.00823400097703957,
      0.1449138983878847,
      0.15718551538837322,
      0.17111321446018563
    ],
    "val_perplexities": [
      53352.66470183977,
      783.2960175762195,
      577.9912141570935,
      468.26364519985043
    ],
    "elapsed_times": [
      0.15465490023295084,
      0.538303009668986,
      0.92034010887146,
      1.2827052474021912
    ],
    "learning_rates": [
      0.024,
      0.024,
      0.024,
      0.024
    ],
    "train_loss_steps": [
      0,
      20,
      40,
      60,
      80,
      100,
      120,
      140
    ],
    "train_loss_tokens": [
      16384,
      344064,
      671744,
      999424,
      1327104,
      1654784,
      1982464,
      2310144
    ],
    "train_losses": [
      10.895920753479004,
      7.1668381690979,
      6.699615478515625,
      6.550482749938965,
      6.510601997375488,
      6.382307529449463,
      6.374586582183838,
      6.36332368850708
    ],
    "train_loss_elapsed_minutes": [
      0.021401381492614745,
      0.25578527053197225,
      0.3558075984319051,
      0.5887209971745809,
      0.6883021275202433,
      0.7878130276997884,
      1.020676561196645,
      1.1201571822166443
    ]
  }
}