# Research Baseline: Muon vs AdamW (88M params, 2B tokens)
# This file serves as a complete, reproducible specification for the 2B token suite.

# Architecture (Explicit for reproducibility)
d_model: 512
n_heads: 8
n_layers: 22
d_ff: 2048
n_kv_heads: 4
max_seq_len: 2048
vocab_size: 49152

# Training setup
batch_size: 8
gradient_accumulation_steps: 1
train_tokens: 2000000000
compile_model: true
use_amp: true

# Optimization
muon_lr: 0.024
muon_momentum: 0.95
adamw_lr: 0.0006
weight_decay: 0.2
grad_clip: 1.0

# Logging & Research
track_manifold: true
detailed_log_every: 500
log_every: 100
save_every: 5000
checkpoint_token_milestone: 200000000
